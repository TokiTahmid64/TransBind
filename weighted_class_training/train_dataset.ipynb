{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_train = []\n",
    "labels_train = []\n",
    "\n",
    "\n",
    "seqs_test= []\n",
    "labels_test = []\n",
    "\n",
    "# Open the file and read the lines\n",
    "with open(\"DNA-735-Train.fasta\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count = 0\n",
    "for line in lines:\n",
    "    if count % 3 == 1:\n",
    "        seqs_train.append(line.strip())\n",
    "    elif count % 3 == 2:\n",
    "        label = line.strip()[1:-1]\n",
    "        # remove the comma from the label\n",
    "\n",
    "        label = label.replace(\",\", \"\")\n",
    "        label = ''.join(label.split())\n",
    "        labels_train.append(label)\n",
    "    count += 1\n",
    "\n",
    "\n",
    "# Open the file and read the lines\n",
    "with open(\"DNA-180-Test.fasta\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "\n",
    "count = 0\n",
    "for line in lines:\n",
    "    if count % 3 == 1:\n",
    "        seqs_test.append(line.strip())\n",
    "    elif count % 3 == 2:\n",
    "        label = line.strip()[1:-1]\n",
    "        # remove the comma from the label\n",
    "\n",
    "        label = label.replace(\",\", \"\")\n",
    "        label = ''.join(label.split())\n",
    "        labels_test.append(label)\n",
    "    count += 1\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import re\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\", do_lower_case=False)\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 735/735 [02:05<00:00,  5.85it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_embedding(seq):\n",
    "\n",
    "    seq = re.sub(r\"[UZOB]\", \"X\", seq)  # Replace ambiguous amino acids with X\n",
    "    seq = \" \".join(list(seq))  # Add spaces between each amino acid\n",
    "    # Ensure the model and tokenizer are on the same device (preferably GPU)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Tokenize the protein sequence\n",
    "    inputs = tokenizer(seq, return_tensors=\"pt\")\n",
    "    inputs = {key: tensor.to(DEVICE) for key, tensor in inputs.items()}\n",
    "\n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # outputs.last_hidden_state has shape [1, sequence_length, hidden_size]\n",
    "    # print(\"Shape of outputs:\", outputs.last_hidden_state.shape)\n",
    "\n",
    "    # Extract the CLS token embedding (first token of the sequence)\n",
    "    return outputs.last_hidden_state.cpu().numpy()[0,:-1,:]\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "embeddings=[]\n",
    "for seq in tqdm(seqs_train):\n",
    "    embeddings.append(generate_embedding(seq))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [01:16<00:00,  2.37it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_test=[]\n",
    "for seq in tqdm(seqs_test):\n",
    "    embeddings_test.append(generate_embedding(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings_train=[]\n",
    "all_labels_train=[]\n",
    "\n",
    "for seq_data in embeddings:\n",
    "    for token_data in seq_data:\n",
    "        all_embeddings_train.append(token_data)\n",
    "\n",
    "for seq_data in labels_train:\n",
    "    for token_data in seq_data:\n",
    "        all_labels_train.append(int(token_data))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_embeddings_test=[]\n",
    "all_labels_test=[]\n",
    "\n",
    "for seq_data in embeddings_test:\n",
    "    for token_data in seq_data:\n",
    "        all_embeddings_test.append(token_data)\n",
    "\n",
    "for seq_data in labels_test:\n",
    "    for token_data in seq_data:\n",
    "        all_labels_test.append(int(token_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_resampled = np.array(all_embeddings_train)\n",
    "y_resampled = np.array(all_labels_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.nn import Embedding, LSTM,Conv1d,Conv2d,MaxPool2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_chanels, **kwargs):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_chanels, **kwargs)\n",
    "        self.bn = nn.BatchNorm1d(out_chanels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.relu(self.bn(self.conv(x)))\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels, \n",
    "        out_1x1,\n",
    "        red_1x1,\n",
    "        out_3x3,\n",
    "        out_5x5,\n",
    "        out_pool,\n",
    "    ):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        self.branch1 = ConvBlock(in_channels, out_1x1, kernel_size=1) # \n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_channels, red_1x1, kernel_size=1, padding=0),\n",
    "            ConvBlock(red_1x1, out_3x3, kernel_size=3, padding=1),\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBlock(in_channels, red_1x1, kernel_size=1),\n",
    "            ConvBlock(red_1x1, out_5x5, kernel_size=5, padding=2),\n",
    "        )\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, padding=1, stride=1),\n",
    "            ConvBlock(in_channels, out_pool, kernel_size=1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        branches = (self.branch1, self.branch2, self.branch3, self.branch4)\n",
    "        #print((self.branch1(x)).shape)\n",
    "        return (torch.cat([branch(x) for branch in branches], 1))\n",
    "\n",
    "class Baseline_1(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "\n",
    "       \n",
    "    ):\n",
    "        super(Baseline_1, self).__init__()\n",
    "        self.inception_1=InceptionBlock(1024,128,128,128,128,128)\n",
    "\n",
    "        self.inception_2=InceptionBlock(512,64,64,64,64,64)\n",
    "\n",
    "        self.inception_3=InceptionBlock(256,32,32,32,32,32)\n",
    "        self.linear_1=nn.Linear(128,32)\n",
    "        self.linear_2=nn.Linear(32,2)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = torch.transpose(x,2,1)\n",
    "        x=self.inception_1(x)\n",
    "        x=self.dropout(x)\n",
    "        x=self.inception_2(x)\n",
    "        x=self.dropout(x)\n",
    "        x=self.inception_3(x)\n",
    "        x=self.dropout(x)\n",
    "        x=torch.mean(x,dim=2)\n",
    "        #x = x.reshape(x.shape[0], -1)\n",
    "        x=self.linear_1(x)\n",
    "        x=F.relu(self.linear_2(x))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalDataset(Dataset):\n",
    "    def __init__(self, data,y) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.y=y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # sample={'data':self.data[index],'y':self.y[index]}\n",
    "        # self.y = int(self.y)\n",
    "        return [self.data[index],self.y[index]]\n",
    "       \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_resampled[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train= MinimalDataset(X_resampled, y_resampled)\n",
    "dataset_test= MinimalDataset(all_embeddings_test,all_labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader =  DataLoader(dataset_train, batch_size= 128, shuffle= True)\n",
    "test_loader = DataLoader(dataset_test, batch_size = 128,  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.3985, Test Loss: 0.3660, Test Accuracy: 85.12%, Test MCC: 0.40, Test Macro F1 Score: 0.41, Test Precision: 0.27, Test AUC: 0.86\n",
      "Epoch 2/20, Train Loss: 0.3488, Test Loss: 0.3661, Test Accuracy: 85.04%, Test MCC: 0.40, Test Macro F1 Score: 0.40, Test Precision: 0.27, Test AUC: 0.85\n",
      "Epoch 3/20, Train Loss: 0.3140, Test Loss: 0.3635, Test Accuracy: 86.27%, Test MCC: 0.41, Test Macro F1 Score: 0.42, Test Precision: 0.29, Test AUC: 0.88\n",
      "Epoch 4/20, Train Loss: 0.2713, Test Loss: 0.3803, Test Accuracy: 89.07%, Test MCC: 0.43, Test Macro F1 Score: 0.45, Test Precision: 0.33, Test AUC: 0.88\n",
      "Epoch 5/20, Train Loss: 0.2345, Test Loss: 0.5365, Test Accuracy: 90.49%, Test MCC: 0.43, Test Macro F1 Score: 0.46, Test Precision: 0.37, Test AUC: 0.84\n",
      "Epoch 6/20, Train Loss: 0.2050, Test Loss: 0.4792, Test Accuracy: 90.43%, Test MCC: 0.43, Test Macro F1 Score: 0.46, Test Precision: 0.36, Test AUC: 0.84\n",
      "Epoch 7/20, Train Loss: 0.1779, Test Loss: 0.5767, Test Accuracy: 91.47%, Test MCC: 0.43, Test Macro F1 Score: 0.47, Test Precision: 0.39, Test AUC: 0.84\n",
      "Epoch 8/20, Train Loss: 0.1576, Test Loss: 0.7502, Test Accuracy: 91.87%, Test MCC: 0.44, Test Macro F1 Score: 0.48, Test Precision: 0.41, Test AUC: 0.85\n",
      "Epoch 9/20, Train Loss: 0.1423, Test Loss: 0.6009, Test Accuracy: 91.17%, Test MCC: 0.43, Test Macro F1 Score: 0.47, Test Precision: 0.39, Test AUC: 0.83\n",
      "Epoch 10/20, Train Loss: 0.1283, Test Loss: 0.8585, Test Accuracy: 92.20%, Test MCC: 0.44, Test Macro F1 Score: 0.47, Test Precision: 0.42, Test AUC: 0.89\n",
      "Epoch 11/20, Train Loss: 0.1200, Test Loss: 0.8903, Test Accuracy: 92.52%, Test MCC: 0.43, Test Macro F1 Score: 0.47, Test Precision: 0.44, Test AUC: 0.82\n",
      "Epoch 12/20, Train Loss: 0.1102, Test Loss: 0.8649, Test Accuracy: 92.70%, Test MCC: 0.43, Test Macro F1 Score: 0.47, Test Precision: 0.45, Test AUC: 0.81\n",
      "Epoch 13/20, Train Loss: 0.1003, Test Loss: 0.9688, Test Accuracy: 92.66%, Test MCC: 0.44, Test Macro F1 Score: 0.48, Test Precision: 0.45, Test AUC: 0.88\n",
      "Epoch 14/20, Train Loss: 0.0911, Test Loss: 0.9817, Test Accuracy: 93.06%, Test MCC: 0.44, Test Macro F1 Score: 0.47, Test Precision: 0.47, Test AUC: 0.80\n",
      "Epoch 15/20, Train Loss: 0.0856, Test Loss: 0.8693, Test Accuracy: 92.36%, Test MCC: 0.44, Test Macro F1 Score: 0.48, Test Precision: 0.43, Test AUC: 0.80\n",
      "Epoch 16/20, Train Loss: 0.0809, Test Loss: 0.9053, Test Accuracy: 92.62%, Test MCC: 0.44, Test Macro F1 Score: 0.47, Test Precision: 0.44, Test AUC: 0.79\n",
      "Epoch 17/20, Train Loss: 0.0765, Test Loss: 1.1229, Test Accuracy: 93.19%, Test MCC: 0.44, Test Macro F1 Score: 0.48, Test Precision: 0.48, Test AUC: 0.80\n",
      "Epoch 18/20, Train Loss: 0.0708, Test Loss: 1.1249, Test Accuracy: 92.87%, Test MCC: 0.45, Test Macro F1 Score: 0.48, Test Precision: 0.46, Test AUC: 0.84\n",
      "Epoch 19/20, Train Loss: 0.0668, Test Loss: 1.0866, Test Accuracy: 93.11%, Test MCC: 0.44, Test Macro F1 Score: 0.48, Test Precision: 0.47, Test AUC: 0.78\n",
      "Epoch 20/20, Train Loss: 0.0619, Test Loss: 1.1167, Test Accuracy: 93.18%, Test MCC: 0.44, Test Macro F1 Score: 0.47, Test Precision: 0.48, Test AUC: 0.85\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Instantiate the model\n",
    "model = Baseline_1().to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming you have a DataLoader for your training data\n",
    "# and your dataset provides (inputs, labels) tuples\n",
    "\n",
    "# Count the frequency of each class\n",
    "class_counts = torch.zeros(2)  # Assuming binary classification; adjust size as needed for more classes\n",
    "for _, labels in DataLoader(dataset_train, batch_size=1, shuffle=False):\n",
    "    class_counts += labels.int().bincount(minlength=class_counts.size(0))\n",
    "\n",
    "# Compute weights as the inverse of the frequency\n",
    "class_weights = 1. / class_counts\n",
    "class_weights = class_weights / class_weights.sum()  # Normalize weights\n",
    "\n",
    "# Move the weights to the correct device\n",
    "class_weights = class_weights.to(device)\n",
    "# Create a weighted loss function\n",
    "criterion = nn.CrossEntropyLoss(class_weights)\n",
    "\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Training Function\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, targets in loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        targets = targets.long()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score, precision_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    all_probs = []  # Store probabilities for AUC calculation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            targets = targets.long()\n",
    "            \n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "            # Append to list for MCC, F1, precision, and AUC calculation\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            # Assuming outputs are logits, apply sigmoid to get probabilities\n",
    "            all_probs.extend(torch.sigmoid(outputs).cpu().numpy()[:, 1])  # Assuming second column has the \"positive\" class probabilities\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(all_targets, all_predictions)\n",
    "    # Calculate Macro F1 Score\n",
    "    f1_macro = f1_score(all_targets, all_predictions)\n",
    "    # Calculate Precision (Weighted)\n",
    "    precision = precision_score(all_targets, all_predictions, zero_division=0)\n",
    "    # Calculate AUC\n",
    "    auc_score = roc_auc_score(all_targets, all_probs)\n",
    "\n",
    "    return total_loss / len(loader), accuracy, mcc, f1_macro, precision, auc_score\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "# Include AUC score in your training/testing loop to print along with other metrics:\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    test_loss, test_acc, test_mcc, test_f1_macro, test_precision, test_auc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%, Test MCC: {test_mcc:.2f}, Test Macro F1 Score: {test_f1_macro:.2f}, Test Precision: {test_precision:.2f}, Test AUC: {test_auc:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
